# -*- coding: utf-8 -*-
"""g25ait1072_CV.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15BVsntri6hvfBoyfL8coZt62h31XuW5j

## Import necessary libraries
"""

import numpy as np
import pandas as pd
import seaborn as sns
from collections import Counter
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap

from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score

"""## Exploratory Data Analysis

### Define functions
"""

class ExploratoryDataAnalysis:

    def __init__(self, df, target_column):
        if not isinstance(df, pd.DataFrame):
            raise TypeError("Input 'df' must be a pandas DataFrame.")
        self.df = df
        self.target_column = target_column
        self.numeric_features = self.df.select_dtypes(include=np.number).columns.tolist()

    def show_summary(self):
        """Prints a comprehensive summary of the dataset."""
        print("--- Dataset Summary ---")
        print(f"\n5 Rows: {self.df.head(5)}")

        print("--- Dataset Summary ---")
        print(f"\nShape of the dataset: {self.df.shape}")

        print("\n--- Data Types and Non-Null Values ---")
        self.df.info()

        print("\n--- Missing Values ---")
        print(self.df.isnull().sum())

        print("\n--- Duplicate Rows ---")
        print(f"Number of duplicate rows: {self.df.duplicated().sum()}")

        print("\n--- Descriptive Statistics ---")
        print(self.df.describe().T)

        print("\n--- Class Distribution ---")
        print(self.df[self.target_column].value_counts())

    def plot_histograms(self):
        """Displays histograms for all numeric features."""
        print("\n---  Displaying Feature Distributions (Histograms) ---")
        self.df[self.numeric_features].hist(bins=20, figsize=(15, 10), layout=(2, 2), edgecolor='black')
        plt.suptitle("Histograms of All Numerical Features", fontsize=16)
        plt.tight_layout(rect=[0, 0, 1, 0.96])
        plt.show()

    def plot_boxplots(self):
        """Displays box plots for numeric features to check for outliers."""
        print("\n--- Displaying Feature Spread and Outliers (Box Plots) ---")
        plt.figure(figsize=(12, 8))
        sns.boxplot(data=self.df[self.numeric_features], orient='h', palette='viridis')
        plt.title("Box Plots of Numerical Features", fontsize=16)
        plt.show()

    def plot_correlation_heatmap(self):
        """Displays a heatmap of the feature correlation matrix."""
        print("\n--- Displaying Feature Correlation (Heatmap) ---")
        correlation_matrix = self.df[self.numeric_features].corr()
        plt.figure(figsize=(10, 8))
        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
        plt.title("Correlation Matrix of Features", fontsize=16)
        plt.show()

    def plot_pairplot(self):
        """Displays a pairplot to visualize relationships between features."""
        print("\n--- Displaying Pairwise Feature Relationships ---")
        sns.pairplot(self.df, hue=self.target_column, palette='viridis', corner=True)
        plt.suptitle("Pairplot of Dataset Features", y=1.02, fontsize=16)
        plt.show()

    def identify_outliers_iqr(self):
        """Identifies and prints outliers for each numeric feature using the IQR method."""
        print("\n--- Outlier Identification (IQR Method) ---")
        for col in self.numeric_features:
            Q1 = self.df[col].quantile(0.25)
            Q3 = self.df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR

            outliers = self.df[(self.df[col] < lower_bound) | (self.df[col] > upper_bound)]

            if not outliers.empty:
                print(f"\nPotential outliers found in '{col}':")
                print(outliers[[col]])
            else:
                print(f"\nNo outliers detected in '{col}'.")

    def run(self):
        """Executes the full EDA process, displaying all info and plots."""
        print(" Starting Comprehensive Exploratory Data Analysis...")
        print("="*50)
        self.show_summary()
        self.plot_histograms()
        self.plot_boxplots()
        self.plot_correlation_heatmap()
        self.plot_pairplot()
        print("="*50)
        print("EDA Complete.")

"""### Load data"""

url = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']
try:
        iris_df = pd.read_csv(url, header=None, names=column_names)
        print("Data loaded successfully.")
except Exception as e:
        print(f"Failed to load data from URL. Please check your internet connection. Error: {e}")
        exit()

"""### Dataset summary"""

eda = ExploratoryDataAnalysis(df=iris_df, target_column='species')
eda.show_summary()

"""### Histograms"""

eda.plot_histograms()

"""### Boxplots"""

eda.plot_boxplots()

"""### Heatmap"""

eda.plot_correlation_heatmap()

"""### Pairplot"""

eda.plot_pairplot()

print("="*50, "\n EDA Complete.")

"""### Outliers detections"""

eda.identify_outliers_iqr()

"""## Part (a): Implementation of K-Fold Cross-Validation

### Define function for cross validation
"""

from sklearn.neighbors import KNeighborsClassifier

def k_fold_cross_validation(model, X, y, k):
    # Shuffle indices
    indices = np.arange(len(X))
    np.random.shuffle(indices)

    # Split into k folds
    fold_sizes = np.full(k, len(X) // k, dtype=int)
    fold_sizes[:len(X) % k] += 1
    current = 0
    folds = []
    for fold_size in fold_sizes:
        start, stop = current, current + fold_size
        folds.append(indices[start:stop])
        current = stop

    # Perform K-Fold Cross-Validation
    scores = []
    for i in range(k):
        # Validation fold
        val_idx = folds[i]
        train_idx = np.hstack([folds[j] for j in range(k) if j != i])

        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]

        model.fit(X_train, y_train)
        y_pred = model.predict(X_val)

        acc = accuracy_score(y_val, y_pred)
        scores.append(acc)

    return scores

"""### Load data"""

def load_and_prepare_data(url):
        column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']
        try:
            df = pd.read_csv(url, header=None, names=column_names)
            print("Data loaded successfully from UCI repository.")
        except Exception as e:
            print(f" Failed to load data. Error: {e}")
            return False

        feature_names = column_names[:-1]
        X_data = df.drop('species', axis=1).values

        le = LabelEncoder()
        y_data = le.fit_transform(df['species'])

        scaler = StandardScaler()
        X_train_split_scaled = scaler.fit_transform(X_data)


        return X_train_split_scaled, y_data

DATA_URL = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
k = 7
X_scaled , y = load_and_prepare_data(DATA_URL)

DATA_URL = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
k = 7
X_scaled , y = load_and_prepare_data(DATA_URL)

"""### Run K-Fold Cross-Validation"""

knn = KNeighborsClassifier(n_neighbors=5)
k = 5
kfold_scores = k_fold_cross_validation(knn, X_scaled, y, k)

print(f"Fold Accuracies: {kfold_scores}")
print(f"Mean Accuracy: {np.mean(kfold_scores):.4f}")
print(f"Standard Deviation: {np.std(kfold_scores):.4f}")

"""## Part (b): Analysis and Comparison

### 1. Comparison with Simple Train-Test Split
"""

from sklearn.model_selection import train_test_split

train_test_scores = []

for random_state in range(5):
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.2, random_state=random_state, stratify=y
    )
    knn.fit(X_train, y_train)
    y_pred = knn.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    train_test_scores.append(acc)

print(f"Train-Test Accuracies: {train_test_scores}")

"""### 2. Visualization of Results

"""

# K-Fold Box Plot
plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
sns.boxplot(y=kfold_scores)
plt.title('K-Fold Cross-Validation Accuracies')
plt.ylabel('Accuracy')

# Train-Test Scatter Plot
plt.subplot(1,2,2)
sns.stripplot(y=train_test_scores, jitter=True, color='orange')
plt.title('Repeated Train-Test Split Accuracies')
plt.ylabel('Accuracy')
plt.show()

"""## Bonus — Stratified K-Fold Cross-Validation

"""

def stratified_k_fold_cross_validation(model, X, y, k):
    unique_classes, y_indices = np.unique(y, return_inverse=True)
    folds = [[] for _ in range(k)]

    # Split indices for each class
    for cls in unique_classes:
        cls_indices = np.where(y == cls)[0]
        np.random.shuffle(cls_indices)
        split_cls = np.array_split(cls_indices, k)
        for i in range(k):
            folds[i].extend(split_cls[i])

    scores = []
    for i in range(k):
        val_idx = np.array(folds[i])
        train_idx = np.array([idx for fold in folds if fold is not folds[i] for idx in fold])

        X_train, X_val = X[train_idx], X[val_idx]
        y_train, y_val = y[train_idx], y[val_idx]

        model.fit(X_train, y_train)
        y_pred = model.predict(X_val)

        scores.append(accuracy_score(y_val, y_pred))

    return scores

# Assuming you have already loaded and standardized X and y
knn = KNeighborsClassifier(n_neighbors=5)
k = 5

# Call the stratified function
stratified_scores = stratified_k_fold_cross_validation(knn, X_scaled, y, k)

print(f"Stratified K-Fold Accuracies: {stratified_scores}")
print(f"Mean Accuracy: {np.mean(stratified_scores):.4f}")
print(f"Standard Deviation: {np.std(stratified_scores):.4f}")

"""## Bonus — Hyperparameter Tuning with Cross-Validation

"""

neighbors_range = range(1, 26)
mean_scores = []

for n in neighbors_range:
    knn = KNeighborsClassifier(n_neighbors=n)
    scores = k_fold_cross_validation(knn, X_scaled, y, k=5)
    mean_scores.append(np.mean(scores))

plt.figure(figsize=(10,6))
plt.plot(neighbors_range, mean_scores, marker='o')
plt.title('Mean CV Accuracy vs Number of Neighbors with k = 5')
plt.xlabel('Number of Neighbors (k)')
plt.ylabel('Mean Cross-Validation Accuracy')
plt.show()

best_k = neighbors_range[np.argmax(mean_scores)]
print(f"Optimal number of neighbors: {best_k} with accuracy {max(mean_scores):.4f}")
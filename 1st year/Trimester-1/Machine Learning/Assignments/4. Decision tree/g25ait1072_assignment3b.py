# -*- coding: utf-8 -*-
"""g25ait1072_DT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xfnThJkkhPvG4KtJEs4DPxEMgyak7izR

## Import necessary libraries
"""

import numpy as np
import pandas as pd
import seaborn as sns
from collections import Counter
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap

from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score
le = LabelEncoder()

"""## Exploratory Data Analysis

### Define functions
"""

class ExploratoryDataAnalysis:

    def __init__(self, df, target_column):
        if not isinstance(df, pd.DataFrame):
            raise TypeError("Input 'df' must be a pandas DataFrame.")
        self.df = df
        self.target_column = target_column
        self.numeric_features = self.df.select_dtypes(include=np.number).columns.tolist()

    def show_summary(self):
        """Prints a comprehensive summary of the dataset."""
        print("--- Dataset Summary ---")
        print(f"\n5 Rows: {self.df.head(5)}")

        print("--- Dataset Summary ---")
        print(f"\nShape of the dataset: {self.df.shape}")

        print("\n--- Data Types and Non-Null Values ---")
        self.df.info()

        print("\n--- Missing Values ---")
        print(self.df.isnull().sum())

        print("\n--- Duplicate Rows ---")
        print(f"Number of duplicate rows: {self.df.duplicated().sum()}")

        print("\n--- Descriptive Statistics ---")
        print(self.df.describe().T)

        print("\n--- Class Distribution ---")
        print(self.df[self.target_column].value_counts())

    def plot_histograms(self):
        """Displays histograms for all numeric features."""
        print("\n---  Displaying Feature Distributions (Histograms) ---")
        self.df[self.numeric_features].hist(bins=20, figsize=(15, 10), layout=(2, 2), edgecolor='black')
        plt.suptitle("Histograms of All Numerical Features", fontsize=16)
        plt.tight_layout(rect=[0, 0, 1, 0.96])
        plt.show()

    def plot_boxplots(self):
        """Displays box plots for numeric features to check for outliers."""
        print("\n--- Displaying Feature Spread and Outliers (Box Plots) ---")
        plt.figure(figsize=(12, 8))
        sns.boxplot(data=self.df[self.numeric_features], orient='h', palette='viridis')
        plt.title("Box Plots of Numerical Features", fontsize=16)
        plt.show()

    def plot_correlation_heatmap(self):
        """Displays a heatmap of the feature correlation matrix."""
        print("\n--- Displaying Feature Correlation (Heatmap) ---")
        correlation_matrix = self.df[self.numeric_features].corr()
        plt.figure(figsize=(10, 8))
        sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f", linewidths=.5)
        plt.title("Correlation Matrix of Features", fontsize=16)
        plt.show()

    def plot_pairplot(self):
        """Displays a pairplot to visualize relationships between features."""
        print("\n--- Displaying Pairwise Feature Relationships ---")
        sns.pairplot(self.df, hue=self.target_column, palette='viridis', corner=True)
        plt.suptitle("Pairplot of Dataset Features", y=1.02, fontsize=16)
        plt.show()

    def identify_outliers_iqr(self):
        """Identifies and prints outliers for each numeric feature using the IQR method."""
        print("\n--- Outlier Identification (IQR Method) ---")
        for col in self.numeric_features:
            Q1 = self.df[col].quantile(0.25)
            Q3 = self.df[col].quantile(0.75)
            IQR = Q3 - Q1
            lower_bound = Q1 - 1.5 * IQR
            upper_bound = Q3 + 1.5 * IQR

            outliers = self.df[(self.df[col] < lower_bound) | (self.df[col] > upper_bound)]

            if not outliers.empty:
                print(f"\nPotential outliers found in '{col}':")
                print(outliers[[col]])
            else:
                print(f"\nNo outliers detected in '{col}'.")

    def run(self):
        """Executes the full EDA process, displaying all info and plots."""
        print(" Starting Comprehensive Exploratory Data Analysis...")
        print("="*50)
        self.show_summary()
        self.plot_histograms()
        self.plot_boxplots()
        self.plot_correlation_heatmap()
        self.plot_pairplot()
        print("="*50)
        print("EDA Complete.")

"""### Load data"""

url = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"
column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']
try:
        iris_df = pd.read_csv(url, header=None, names=column_names)
        print("Data loaded successfully.")
except Exception as e:
        print(f"Failed to load data from URL. Please check your internet connection. Error: {e}")
        exit()

"""### Dataset summary"""

eda = ExploratoryDataAnalysis(df=iris_df, target_column='species')
eda.show_summary()

"""### Histograms"""

eda.plot_histograms()

"""### Boxplots"""

eda.plot_boxplots()

"""### Heatmap"""

eda.plot_correlation_heatmap()

"""### Pairplot"""

eda.plot_pairplot()

print("="*50, "\n EDA Complete.")

"""### Outliers detections"""

eda.identify_outliers_iqr()

"""## Part (a): Implementation of the Decision Tree Model

### Step 1: Load the dataset
"""

def load_and_prepare_data(url):
        column_names = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'species']
        try:
            df = pd.read_csv(url, header=None, names=column_names)
            print("Data loaded successfully from UCI repository.")
        except Exception as e:
            print(f" Failed to load data. Error: {e}")
            return False

        feature_names = column_names[:-1]
        X_data = df.drop('species', axis=1).values

        y_data = le.fit_transform(df['species'])

        return X_data, y_data

"""### Step 2: Define impurity measures and information gain functions

"""

def gini_impurity(y):
    m = len(y)
    if m == 0:
        return 0
    class_counts = Counter(y)
    impurity = 1.0
    for lbl in class_counts:
        p = class_counts[lbl] / m
        impurity -= p**2
    return impurity

def entropy(y):
    m = len(y)
    if m == 0:
        return 0
    class_counts = Counter(y)
    ent = 0.0
    for lbl in class_counts:
        p = class_counts[lbl] / m
        ent -= p * np.log2(p)
    return ent

def information_gain(parent_y, left_y, right_y, criterion='gini'):
    if criterion == 'gini':
        impurity_func = gini_impurity
    else:
        impurity_func = entropy

    parent_impurity = impurity_func(parent_y)
    n = len(parent_y)
    n_left = len(left_y)
    n_right = len(right_y)

    if n == 0:
        return 0

    weighted_impurity = (n_left / n) * impurity_func(left_y) + (n_right / n) * impurity_func(right_y)
    gain = parent_impurity - weighted_impurity
    return gain

"""### Step 3: Define the Node class for the tree structure

"""

class Node:
    def __init__(self, feature_index=None, threshold=None, left=None, right=None, *, value=None):
        self.feature_index = feature_index  # index of feature to split on
        self.threshold = threshold  # threshold for splitting
        self.left = left    # left child
        self.right = right  # right child
        self.value = value  # class if leaf node

    def is_leaf_node(self):
        return self.value is not None

"""### Step 4: Define the DecisionTreeClassifier class

"""

class DecisionTreeClassifier:
    def __init__(self, max_depth=3, min_samples_split=2, criterion='gini', min_impurity_decrease=0.0):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.criterion = criterion  # 'gini' or 'entropy'
        self.min_impurity_decrease = min_impurity_decrease
        self.root = None

    def fit(self, X, y):
        self.n_classes_ = len(np.unique(y))
        self.n_features_ = X.shape[1]
        self.root = self._grow_tree(X, y)

    def _grow_tree(self, X, y, depth=0):
        n_samples, n_features = X.shape
        num_labels = len(np.unique(y))

        if (depth >= self.max_depth or
            n_samples < self.min_samples_split or
            num_labels == 1):
            leaf_value = self._most_common_label(y)
            return Node(value=leaf_value)

        # Find best split
        feature_idx, threshold, gain = self._best_split(X, y)

        if gain is None or gain < self.min_impurity_decrease:
            leaf_value = self._most_common_label(y)
            return Node(value=leaf_value)

        # Partition dataset
        left_idxs = X[:, feature_idx] < threshold
        right_idxs = X[:, feature_idx] >= threshold

        left = self._grow_tree(X[left_idxs], y[left_idxs], depth + 1)
        right = self._grow_tree(X[right_idxs], y[right_idxs], depth + 1)

        return Node(feature_index=feature_idx, threshold=threshold, left=left, right=right)

    def _best_split(self, X, y):
        best_gain = -float('inf')
        split_idx, split_thresh = None, None

        for feature_idx in range(self.n_features_):
            X_column = X[:, feature_idx]
            thresholds = np.unique(X_column)

            for threshold in thresholds:
                left_mask = X_column < threshold
                right_mask = X_column >= threshold

                if sum(left_mask) == 0 or sum(right_mask) == 0:
                    continue

                y_left, y_right = y[left_mask], y[right_mask]

                gain = information_gain(y, y_left, y_right, criterion=self.criterion)

                if gain > best_gain:
                    best_gain = gain
                    split_idx = feature_idx
                    split_thresh = threshold

        if best_gain == -float('inf'):
            return None, None, None
        return split_idx, split_thresh, best_gain

    def _most_common_label(self, y):
        counter = Counter(y)
        most_common = counter.most_common(1)[0][0]
        return most_common

    def predict(self, X):
        return np.array([self._traverse_tree(x, self.root) for x in X])

    def _traverse_tree(self, x, node):
        if node.is_leaf_node():
            return node.value

        if x[node.feature_index] < node.threshold:
            return self._traverse_tree(x, node.left)
        else:
            return self._traverse_tree(x, node.right)

"""### Step 5: Data preparation and model training

"""

# Load dataset
DATA_URL = "https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data"

X, y_encoded = load_and_prepare_data(DATA_URL)

# Split into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.2, random_state=42)

# Instantiate and train model
clf = DecisionTreeClassifier(max_depth=3)
clf.fit(X_train, y_train)

"""## Part (b): Evaluation and Visualization

### Step 1: Evaluate model accuracy
"""

from sklearn.metrics import accuracy_score

y_train_pred = clf.predict(X_train)
y_test_pred = clf.predict(X_test)

train_acc = accuracy_score(y_train, y_train_pred)
test_acc = accuracy_score(y_test, y_test_pred)

print(f"Training accuracy: {train_acc:.4f}")
print(f"Testing accuracy: {test_acc:.4f}")

"""### Step 2: Analyze effect of max_depth

"""

depths = [1, 2, 3, 5, 6, 7 , 8 , 9 ,10]
train_scores = []
test_scores = []
for depth in depths:
    model = DecisionTreeClassifier(max_depth=depth)
    model.fit(X_train, y_train)
    train_pred = model.predict(X_train)
    test_pred = model.predict(X_test)
    train_scores.append(accuracy_score(y_train, train_pred))
    test_scores.append(accuracy_score(y_test, test_pred))

import pandas as pd
results_df = pd.DataFrame({
    'max_depth': depths,
    'train_accuracy': train_scores,
    'test_accuracy': test_scores
})
results_df

from google.colab import sheets
sheet = sheets.InteractiveSheet(df=results_df)

"""### Step 3: Function to print tree structure textually

"""

def print_tree(node, depth=0):
    indent = "  " * depth
    if node.is_leaf_node():
        print(f"{indent}Predict: {le.inverse_transform([node.value])[0]}")
    else:
        print(f"{indent}Feature {node.feature_index} < {node.threshold:.3f}?")
        print(f"{indent}--> True:")
        print_tree(node.left, depth + 1)
        print(f"{indent}--> False:")
        print_tree(node.right, depth + 1)

print_tree(clf.root)

from graphviz import Digraph

def plot_tree_graphviz(node, feature_names, class_names, dot=None, parent=None, edge_label=None):
    if dot is None:
        dot = Digraph()
        dot.attr(rankdir='TB')  # Top to Bottom

    if node.is_leaf_node():
        label = f'Predict\n{class_names[node.value]}'
        node_id = str(id(node))
        dot.node(node_id, label=label, shape='box', style='filled', color='lightgrey')
    else:
        node_id = str(id(node))
        label = f'{feature_names[node.feature_index]} < {node.threshold:.2f}'
        dot.node(node_id, label=label, shape='ellipse')

        left_id = plot_tree_graphviz(node.left, feature_names, class_names, dot, node_id, 'True')
        right_id = plot_tree_graphviz(node.right, feature_names, class_names, dot, node_id, 'False')

        dot.edge(node_id, left_id, label='True')
        dot.edge(node_id, right_id, label='False')

    if parent:
        return node_id
    else:
        return dot

# Usage example:
dot = plot_tree_graphviz(clf.root, feature_names, class_names)
dot.render('decision_tree', format='png', cleanup=True)  # Saves tree as 'decision_tree.png'

"""### Step 4: Decision boundary plot using two features

"""

from matplotlib.colors import ListedColormap

# Select only two features: petal length and petal width (indices 2 and 3)
X_2d = X[:, 2:4]
X_train_2d, X_test_2d, y_train_2d, y_test_2d = train_test_split(X_2d, y_encoded, test_size=0.2, random_state=42)

clf_2d = DecisionTreeClassifier(max_depth=3)
clf_2d.fit(X_train_2d, y_train_2d)

# Plot decision boundary
x_min, x_max = X_2d[:, 0].min() - 1, X_2d[:, 0].max() + 1
y_min, y_max = X_2d[:, 1].min() - 1, X_2d[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))

Z = clf_2d.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)

cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])

plt.figure(figsize=(10, 6))
plt.contourf(xx, yy, Z, alpha=0.4, cmap=cmap_light)

# Plot training points
for idx, class_label in enumerate(np.unique(y_train_2d)):
    plt.scatter(X_train_2d[y_train_2d == class_label, 0],
                X_train_2d[y_train_2d == class_label, 1],
                c=cmap_bold(idx), label=le.inverse_transform([class_label])[0])

plt.xlabel('Petal length')
plt.ylabel('Petal width')
plt.title('Decision Tree Decision Boundary (depth=3)')
plt.legend()
plt.show()
# -*- coding: utf-8 -*-
"""g25ait1072

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1E29f3EUrDk0wmnPRjGTIeZc3oUPHWFck

## Import necessary libraries
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.datasets import fetch_california_housing
import os

"""## Part (a): Implementation of the Linear Regression Model

### Linear regression model from scratch
"""

class CustomLinearRegression:

    def __init__(self, learning_rate=0.01, n_iterations=1000, lambda_=0):

        self.learning_rate = learning_rate
        self.n_iterations = n_iterations
        self.lambda_ = lambda_  # L2 regularization parameter
        self.weights = None
        self.bias = None
        self.cost_history = []  # To store the cost at each iteration

    def fit(self, X, y):
        """
        Trains the model by finding the optimal weights and bias using Gradient Descent.
        """
        n_samples, n_features = X.shape

        # 1. Initialize parameters
        self.weights = np.zeros(n_features)
        self.bias = 0
        self.cost_history = []

        print(f"   - Starting custom Gradient Descent (lr={self.learning_rate}, iter={self.n_iterations}, lambda={self.lambda_})...")
        # 2. Gradient Descent loop
        for i in range(self.n_iterations):
            y_predicted = np.dot(X, self.weights) + self.bias

            # Calculate cost (MSE) and store it
            cost = np.mean((y_predicted - y)**2)
            self.cost_history.append(cost)

            # Calculate the gradients
            # The regularization term is added to the weight gradient
            regularization_term = (self.lambda_ / n_samples) * self.weights
            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y)) + regularization_term
            db = (1 / n_samples) * np.sum(y_predicted - y)

            # 3. Update the weights and bias
            self.weights -= self.learning_rate * dw
            self.bias -= self.learning_rate * db

            if (i + 1) % 100 == 0:
                print(f"   - Iteration {i+1}/{self.n_iterations}, Cost (MSE): {cost:.4f}")

    def predict(self, X):
        """
        Makes a prediction using the trained weights and bias.
        """
        return np.dot(X, self.weights) + self.bias

"""### The encpsulated pipeline design pattern"""

class RegressionPipeline:
    """
    An encapsulated pipeline that uses the custom linear regression model.
    """
    def __init__(self, learning_rate=0.1, n_iterations=1000, lambda_=0):
        """Initializes the pipeline with hyperparameters for the custom model."""
        self.model = CustomLinearRegression(learning_rate, n_iterations, lambda_)
        self.scaler = StandardScaler()
        self.features = None
        self.target = None
        self.train_metrics = {}
        self.test_metrics = {}

    def _load_data(self):
        """Loads and prepares the California Housing dataset."""
        print("1. Loading California Housing dataset...")
        housing = fetch_california_housing()
        df = pd.DataFrame(housing.data, columns=housing.feature_names)
        df['MedHouseVal'] = housing.target
        self.features = housing.feature_names
        self.target = 'MedHouseVal'
        return df

    def _preprocess_data(self, df: pd.DataFrame):
        """Preprocesses the data: handles missing values, splits, and scales."""
        print("2. Preprocessing data...")
        df.dropna(inplace=True)
        X = df[self.features]
        y = df[self.target]
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

        # Standardize features
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_test_scaled = self.scaler.transform(X_test)

        print("   - Data split into 80% training and 20% testing sets.")
        print("   - Features standardized (mean=0, variance=1).")
        return X_train_scaled, X_test_scaled, y_train.to_numpy(), y_test.to_numpy()

    def perform_eda(self, df: pd.DataFrame):
        """Performs Exploratory Data Analysis (EDA)."""
        print("\n--- [ANALYSIS] Performing Exploratory Data Analysis ---")
        print("\nData Head:\n", df.head())
        print("\nData Description:\n", df.describe())

        plt.figure(figsize=(10, 8))
        sns.heatmap(df.corr(), annot=True, cmap='viridis', fmt=".2f")
        plt.title("Correlation Heatmap")
        plt.show()

    def train(self, X_train, y_train):
        """Trains the custom linear regression model."""
        print("3. Training the model...")
        self.model.fit(X_train, y_train)
        print("   - Model training complete.")

    def _evaluate(self, X, y_true, dataset_name: str):
        """Evaluates the model and prints metrics (Part b)."""
        print(f"4. Evaluating model on {dataset_name} set...")
        y_pred = self.model.predict(X)
        mse = mean_squared_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)

        metrics = {"MSE": mse, "R-squared": r2}
        print(f"   - {dataset_name} MSE: {mse:.4f}")
        print(f"   - {dataset_name} R-squared: {r2:.4f}")
        return metrics, y_pred

    def plot_learning_curve(self):
        """Plots the learning curve (Cost vs. Iterations) (Part b)."""
        print("\n--- [VISUALIZATION] Plotting learning curve ---")
        plt.figure(figsize=(10, 6))
        plt.plot(range(1, self.model.n_iterations + 1), self.model.cost_history)
        plt.xlabel("Number of Iterations")
        plt.ylabel("Cost (MSE)")
        plt.title("Learning Curve: Cost vs. Iterations")
        plt.grid(True)
        plt.show()

    def run_full_pipeline(self):
        """Runs the entire machine learning pipeline."""
        print("\n--- [WORKFLOW] Starting Full Regression Pipeline ---")
        data = self._load_data()
        X_train, X_test, y_train, y_test = self._preprocess_data(data)
        self.train(X_train, y_train)
        self.train_metrics, _ = self._evaluate(X_train, y_train, "Training")
        self.test_metrics, y_pred_test = self._evaluate(X_test, y_test, "Testing")
        self.plot_learning_curve()
        print("--- Pipeline Finished ---")
        return y_test, y_pred_test, data

    def predict(self, new_data: pd.DataFrame):
        """Makes predictions on new, unseen data."""
        new_data_scaled = self.scaler.transform(new_data[self.features])
        return self.model.predict(new_data_scaled)

"""## Part (a): Evaluation and Visualization of the Linear Regression Model

### EDA
"""

pipeline = RegressionPipeline(learning_rate=0.1, n_iterations=1000, lambda_=0)
df = pipeline._load_data()
pipeline.perform_eda(df)

"""###  1. Instantiate and Run the Main Pipeline"""

y_test_actual, y_test_predicted, full_data = pipeline.run_full_pipeline()

"""### 2. Visualize Test Results"""

if y_test_actual is not None and y_test_predicted is not None:
        print("\n--- [VISUALIZATION] Plotting test set predictions vs actuals ---")
        plt.figure(figsize=(10, 6))
        plt.scatter(y_test_actual, y_test_predicted, alpha=0.7, edgecolors='k')
        plt.plot([y_test_actual.min(), y_test_actual.max()], [y_test_actual.min(), y_test_actual.max()], 'r--', lw=2)
        plt.xlabel("Actual Median House Value ($100,000s)")
        plt.ylabel("Predicted Median House Value ($100,000s)")
        plt.title("Actual vs. Predicted Values (Test Set)")
        plt.grid(True)
        plt.show()

"""## Part(c): Bonus Challenge:

### 1. Bonus Challenge: Implement L2 Regularization
"""

print("\n" + "="*50)
print("BONUS CHALLENGE: L2 Regularization (Ridge)")
print("="*50)
pipeline_l2 = RegressionPipeline(learning_rate=0.1, n_iterations=1000, lambda_=1.0)
_, _, _ = pipeline_l2.run_full_pipeline()
print("Observation: Compare the MSE and R-squared of this regularized model")
print("with the non-regularized one above in your report.")

"""### 2. Bonus Challenge: Experiment with Learning Rates"""

print("\n" + "="*50)
print("BONUS CHALLENGE: Experimenting with Learning Rates")
print("="*50)
# Load and preprocess data once for this experiment
X_train_lr, _, y_train_lr, _ = pipeline._preprocess_data(full_data)
learning_rates = {
        "Too High": 1.1,
        "Too Low": 0.0001,
        "Just Right": 0.1
    }
plt.figure(figsize=(12, 7))
for name, lr in learning_rates.items():
        model_lr = CustomLinearRegression(learning_rate=lr, n_iterations=1000)
        model_lr.fit(X_train_lr, y_train_lr)
        plt.plot(model_lr.cost_history, label=f"LR = {lr} ({name})")
plt.xlabel("Number of Iterations")
plt.ylabel("Cost (MSE)")
plt.title("Effect of Different Learning Rates on Convergence")
plt.legend()
plt.grid(True)
plt.ylim(0, 5) # Limit y-axis to see the convergence details
plt.show()



print("Observation: Analyze the plot in your report. The high learning rate diverges (cost explodes),")
print("the low rate converges very slowly, and the 'just right' rate converges smoothly.")